{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/westhomas/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ta\n",
    "import os\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from binance.helpers import round_step_size\n",
    "from binance.client import Client\n",
    "from binance.enums import *\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta, date, time\n",
    "from itertools import product\n",
    "from time import strftime\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import os.path\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import ta\n",
    "import ast\n",
    "\n",
    "\n",
    "import importlib.util\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>EMA_4</th>\n",
       "      <th>EMA_8</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>...</th>\n",
       "      <th>FIB_618</th>\n",
       "      <th>FIB_100</th>\n",
       "      <th>FIB_1618</th>\n",
       "      <th>FIB_2618</th>\n",
       "      <th>FIB_4236</th>\n",
       "      <th>STD_2</th>\n",
       "      <th>STD_5</th>\n",
       "      <th>STD_10</th>\n",
       "      <th>STD_20</th>\n",
       "      <th>STD_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-14</td>\n",
       "      <td>16320.04</td>\n",
       "      <td>16326.99</td>\n",
       "      <td>15670.00</td>\n",
       "      <td>16070.45</td>\n",
       "      <td>15957.00</td>\n",
       "      <td>59116.34718</td>\n",
       "      <td>15990.54133</td>\n",
       "      <td>15592.32200</td>\n",
       "      <td>15234.64674</td>\n",
       "      <td>...</td>\n",
       "      <td>16278.43770</td>\n",
       "      <td>16480.00</td>\n",
       "      <td>16806.08770</td>\n",
       "      <td>17333.73770</td>\n",
       "      <td>18187.47540</td>\n",
       "      <td>20.392960</td>\n",
       "      <td>500.111565</td>\n",
       "      <td>641.311590</td>\n",
       "      <td>1111.150072</td>\n",
       "      <td>1506.611616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>16069.56</td>\n",
       "      <td>16180.00</td>\n",
       "      <td>15774.72</td>\n",
       "      <td>15957.00</td>\n",
       "      <td>16713.57</td>\n",
       "      <td>43596.84151</td>\n",
       "      <td>16022.50480</td>\n",
       "      <td>15698.57267</td>\n",
       "      <td>15363.23186</td>\n",
       "      <td>...</td>\n",
       "      <td>16076.01982</td>\n",
       "      <td>16326.99</td>\n",
       "      <td>16733.00982</td>\n",
       "      <td>17389.99982</td>\n",
       "      <td>18453.00964</td>\n",
       "      <td>176.953472</td>\n",
       "      <td>437.062551</td>\n",
       "      <td>471.202503</td>\n",
       "      <td>1109.834135</td>\n",
       "      <td>1508.570584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>15957.00</td>\n",
       "      <td>16880.00</td>\n",
       "      <td>15864.00</td>\n",
       "      <td>16713.57</td>\n",
       "      <td>17659.38</td>\n",
       "      <td>81300.67592</td>\n",
       "      <td>15996.30288</td>\n",
       "      <td>15756.00096</td>\n",
       "      <td>15454.58080</td>\n",
       "      <td>...</td>\n",
       "      <td>16025.18304</td>\n",
       "      <td>16180.00</td>\n",
       "      <td>16430.46304</td>\n",
       "      <td>16835.74304</td>\n",
       "      <td>17491.48608</td>\n",
       "      <td>80.221264</td>\n",
       "      <td>261.495563</td>\n",
       "      <td>480.588085</td>\n",
       "      <td>1081.531916</td>\n",
       "      <td>1478.130989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-17</td>\n",
       "      <td>16713.08</td>\n",
       "      <td>17858.82</td>\n",
       "      <td>16538.00</td>\n",
       "      <td>17659.38</td>\n",
       "      <td>17776.12</td>\n",
       "      <td>115221.40310</td>\n",
       "      <td>16283.20973</td>\n",
       "      <td>15968.79408</td>\n",
       "      <td>15648.27145</td>\n",
       "      <td>...</td>\n",
       "      <td>16491.88800</td>\n",
       "      <td>16880.00</td>\n",
       "      <td>17507.88800</td>\n",
       "      <td>18523.88800</td>\n",
       "      <td>20167.77600</td>\n",
       "      <td>534.975777</td>\n",
       "      <td>290.512191</td>\n",
       "      <td>577.654357</td>\n",
       "      <td>1128.133906</td>\n",
       "      <td>1471.617427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-18</td>\n",
       "      <td>17659.38</td>\n",
       "      <td>18476.93</td>\n",
       "      <td>17214.45</td>\n",
       "      <td>17776.12</td>\n",
       "      <td>17802.82</td>\n",
       "      <td>149019.78810</td>\n",
       "      <td>16833.67784</td>\n",
       "      <td>16344.47984</td>\n",
       "      <td>15957.67276</td>\n",
       "      <td>...</td>\n",
       "      <td>17354.26676</td>\n",
       "      <td>17858.82</td>\n",
       "      <td>18675.08676</td>\n",
       "      <td>19995.90676</td>\n",
       "      <td>22132.99352</td>\n",
       "      <td>668.788665</td>\n",
       "      <td>687.660487</td>\n",
       "      <td>723.730363</td>\n",
       "      <td>1210.994287</td>\n",
       "      <td>1514.345975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>2025-03-02</td>\n",
       "      <td>97700.59</td>\n",
       "      <td>102500.01</td>\n",
       "      <td>91231.00</td>\n",
       "      <td>101328.52</td>\n",
       "      <td>97763.13</td>\n",
       "      <td>75164.73850</td>\n",
       "      <td>100368.35640</td>\n",
       "      <td>101483.79470</td>\n",
       "      <td>101736.11850</td>\n",
       "      <td>...</td>\n",
       "      <td>99429.47880</td>\n",
       "      <td>101456.60</td>\n",
       "      <td>104736.07880</td>\n",
       "      <td>110042.67880</td>\n",
       "      <td>118628.75760</td>\n",
       "      <td>2075.400829</td>\n",
       "      <td>2776.164584</td>\n",
       "      <td>2240.733899</td>\n",
       "      <td>2464.809730</td>\n",
       "      <td>3911.172648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>101328.51</td>\n",
       "      <td>101732.31</td>\n",
       "      <td>96150.00</td>\n",
       "      <td>97763.13</td>\n",
       "      <td>96612.43</td>\n",
       "      <td>40267.98697</td>\n",
       "      <td>100752.42180</td>\n",
       "      <td>101449.28920</td>\n",
       "      <td>101673.41110</td>\n",
       "      <td>...</td>\n",
       "      <td>98195.24818</td>\n",
       "      <td>102500.01</td>\n",
       "      <td>109464.25820</td>\n",
       "      <td>120733.26820</td>\n",
       "      <td>138966.52640</td>\n",
       "      <td>2565.333905</td>\n",
       "      <td>2567.597122</td>\n",
       "      <td>2097.614453</td>\n",
       "      <td>2069.702243</td>\n",
       "      <td>3894.676064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>97763.14</td>\n",
       "      <td>99149.00</td>\n",
       "      <td>96155.00</td>\n",
       "      <td>96612.43</td>\n",
       "      <td>96554.35</td>\n",
       "      <td>26233.30444</td>\n",
       "      <td>99556.70509</td>\n",
       "      <td>100630.14270</td>\n",
       "      <td>101071.82940</td>\n",
       "      <td>...</td>\n",
       "      <td>99599.86758</td>\n",
       "      <td>101732.31</td>\n",
       "      <td>105182.17760</td>\n",
       "      <td>110764.48760</td>\n",
       "      <td>119796.66520</td>\n",
       "      <td>2521.111447</td>\n",
       "      <td>2142.346129</td>\n",
       "      <td>2285.076135</td>\n",
       "      <td>2296.165594</td>\n",
       "      <td>3907.169509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>96612.44</td>\n",
       "      <td>99120.00</td>\n",
       "      <td>95676.64</td>\n",
       "      <td>96554.35</td>\n",
       "      <td>96506.80</td>\n",
       "      <td>23515.20405</td>\n",
       "      <td>98378.99505</td>\n",
       "      <td>99737.31768</td>\n",
       "      <td>100385.76790</td>\n",
       "      <td>...</td>\n",
       "      <td>98005.29200</td>\n",
       "      <td>99149.00</td>\n",
       "      <td>100999.29200</td>\n",
       "      <td>103993.29200</td>\n",
       "      <td>108837.58400</td>\n",
       "      <td>813.667773</td>\n",
       "      <td>2051.338796</td>\n",
       "      <td>2692.384366</td>\n",
       "      <td>2594.895390</td>\n",
       "      <td>3951.057412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2025-07-02</td>\n",
       "      <td>96554.35</td>\n",
       "      <td>100137.99</td>\n",
       "      <td>95620.34</td>\n",
       "      <td>96506.80</td>\n",
       "      <td>96444.74</td>\n",
       "      <td>31794.22065</td>\n",
       "      <td>97649.13703</td>\n",
       "      <td>99029.99153</td>\n",
       "      <td>99796.31900</td>\n",
       "      <td>...</td>\n",
       "      <td>97804.63648</td>\n",
       "      <td>99120.00</td>\n",
       "      <td>101247.99650</td>\n",
       "      <td>104691.35650</td>\n",
       "      <td>110262.71300</td>\n",
       "      <td>41.068762</td>\n",
       "      <td>1951.904051</td>\n",
       "      <td>2961.694277</td>\n",
       "      <td>2859.762229</td>\n",
       "      <td>3963.120298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1547 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE       OPEN       HIGH       LOW      CLOSE    TARGET  \\\n",
       "0    2020-11-14   16320.04   16326.99  15670.00   16070.45  15957.00   \n",
       "1    2020-11-15   16069.56   16180.00  15774.72   15957.00  16713.57   \n",
       "2    2020-11-16   15957.00   16880.00  15864.00   16713.57  17659.38   \n",
       "3    2020-11-17   16713.08   17858.82  16538.00   17659.38  17776.12   \n",
       "4    2020-11-18   17659.38   18476.93  17214.45   17776.12  17802.82   \n",
       "...         ...        ...        ...       ...        ...       ...   \n",
       "1542 2025-03-02   97700.59  102500.01  91231.00  101328.52  97763.13   \n",
       "1543 2025-04-02  101328.51  101732.31  96150.00   97763.13  96612.43   \n",
       "1544 2025-05-02   97763.14   99149.00  96155.00   96612.43  96554.35   \n",
       "1545 2025-06-02   96612.44   99120.00  95676.64   96554.35  96506.80   \n",
       "1546 2025-07-02   96554.35  100137.99  95620.34   96506.80  96444.74   \n",
       "\n",
       "            VOLUME         EMA_4         EMA_8        EMA_12  ...  \\\n",
       "0      59116.34718   15990.54133   15592.32200   15234.64674  ...   \n",
       "1      43596.84151   16022.50480   15698.57267   15363.23186  ...   \n",
       "2      81300.67592   15996.30288   15756.00096   15454.58080  ...   \n",
       "3     115221.40310   16283.20973   15968.79408   15648.27145  ...   \n",
       "4     149019.78810   16833.67784   16344.47984   15957.67276  ...   \n",
       "...            ...           ...           ...           ...  ...   \n",
       "1542   75164.73850  100368.35640  101483.79470  101736.11850  ...   \n",
       "1543   40267.98697  100752.42180  101449.28920  101673.41110  ...   \n",
       "1544   26233.30444   99556.70509  100630.14270  101071.82940  ...   \n",
       "1545   23515.20405   98378.99505   99737.31768  100385.76790  ...   \n",
       "1546   31794.22065   97649.13703   99029.99153   99796.31900  ...   \n",
       "\n",
       "          FIB_618    FIB_100      FIB_1618      FIB_2618      FIB_4236  \\\n",
       "0     16278.43770   16480.00   16806.08770   17333.73770   18187.47540   \n",
       "1     16076.01982   16326.99   16733.00982   17389.99982   18453.00964   \n",
       "2     16025.18304   16180.00   16430.46304   16835.74304   17491.48608   \n",
       "3     16491.88800   16880.00   17507.88800   18523.88800   20167.77600   \n",
       "4     17354.26676   17858.82   18675.08676   19995.90676   22132.99352   \n",
       "...           ...        ...           ...           ...           ...   \n",
       "1542  99429.47880  101456.60  104736.07880  110042.67880  118628.75760   \n",
       "1543  98195.24818  102500.01  109464.25820  120733.26820  138966.52640   \n",
       "1544  99599.86758  101732.31  105182.17760  110764.48760  119796.66520   \n",
       "1545  98005.29200   99149.00  100999.29200  103993.29200  108837.58400   \n",
       "1546  97804.63648   99120.00  101247.99650  104691.35650  110262.71300   \n",
       "\n",
       "            STD_2        STD_5       STD_10       STD_20       STD_30  \n",
       "0       20.392960   500.111565   641.311590  1111.150072  1506.611616  \n",
       "1      176.953472   437.062551   471.202503  1109.834135  1508.570584  \n",
       "2       80.221264   261.495563   480.588085  1081.531916  1478.130989  \n",
       "3      534.975777   290.512191   577.654357  1128.133906  1471.617427  \n",
       "4      668.788665   687.660487   723.730363  1210.994287  1514.345975  \n",
       "...           ...          ...          ...          ...          ...  \n",
       "1542  2075.400829  2776.164584  2240.733899  2464.809730  3911.172648  \n",
       "1543  2565.333905  2567.597122  2097.614453  2069.702243  3894.676064  \n",
       "1544  2521.111447  2142.346129  2285.076135  2296.165594  3907.169509  \n",
       "1545   813.667773  2051.338796  2692.384366  2594.895390  3951.057412  \n",
       "1546    41.068762  1951.904051  2961.694277  2859.762229  3963.120298  \n",
       "\n",
       "[1547 rows x 70 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EXPORT_RAW_DATA():\n",
    "\n",
    "    def __init__(self, DF, FILEPATH):\n",
    "        self.DF                     = DF\n",
    "        self.FILEPATH               = FILEPATH\n",
    "        self.DF                     = self.READ_IN_RAW_DATA()\n",
    "\n",
    "    def READ_IN_RAW_DATA(self):\n",
    "\n",
    "        self.DF.to_csv(self.FILEPATH, index=False)\n",
    "        print(f\"Data has been exported to - {self.FILEPATH}\")\n",
    "\n",
    "        return self.DF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class READ_IN_DATA_LOWER():\n",
    "\n",
    "    def __init__(self, COINS, TIMEFRAME, PATH):\n",
    "        self.PATH                               = PATH\n",
    "        self.COINS                              = COINS\n",
    "        self.TIMEFRAME                          = TIMEFRAME\n",
    "        self.LIST                               = self.READ_IN_RAW_DATA()\n",
    "\n",
    "\n",
    "    def READ_IN_RAW_DATA(self):\n",
    "        COIN_HIERARCHY = []\n",
    "\n",
    "        for a in range(len(self.COINS)):\n",
    "            TIME_FRAME_HIERARCHY = []\n",
    "            for b in range(len(self.TIMEFRAME)):\n",
    "\n",
    "                \n",
    "                FILENAME                    = self.COINS[a] + ' -- ' + self.TIMEFRAME[b] + '.csv'\n",
    "                FILE_PATH                   = self.PATH + FILENAME\n",
    "                DF                          = (pd.read_csv(FILE_PATH)).iloc[:-1]\n",
    "                DF[\"DATE\"]                  = pd.to_datetime(DF[\"DATE\"], format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
    "                DF                          = DF[['DATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME']]\n",
    "\n",
    "                TIME_FRAME_HIERARCHY.append(DF)\n",
    "\n",
    "            COIN_HIERARCHY.append(TIME_FRAME_HIERARCHY)\n",
    "\n",
    "        return COIN_HIERARCHY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class READ_IN_DATA():\n",
    "\n",
    "    def __init__(self, COINS, TIMEFRAME, PATH):\n",
    "        self.PATH                               = PATH\n",
    "        self.COINS                              = COINS\n",
    "        self.TIMEFRAME                          = TIMEFRAME\n",
    "        self.LIST                               = self.READ_IN_RAW_DATA()\n",
    "\n",
    "\n",
    "    def READ_IN_RAW_DATA(self):\n",
    "        COIN_HIERARCHY = []\n",
    "\n",
    "        for a in range(len(self.COINS)):\n",
    "            TIME_FRAME_HIERARCHY = []\n",
    "            for b in range(len(self.TIMEFRAME)):\n",
    "\n",
    "                \n",
    "                FILENAME                    = self.COINS[a] + ' -- ' + self.TIMEFRAME[b] + '.csv'\n",
    "                FILE_PATH                   = self.PATH + FILENAME\n",
    "                DF                          = (pd.read_csv(FILE_PATH)).iloc[:-1]\n",
    "                DF[\"DATE\"]                  = pd.to_datetime(DF[\"DATE\"])\n",
    "\n",
    "                TIME_FRAME_HIERARCHY.append(DF)\n",
    "\n",
    "            COIN_HIERARCHY.append(TIME_FRAME_HIERARCHY)\n",
    "\n",
    "        return COIN_HIERARCHY\n",
    "\n",
    "\n",
    "\n",
    "class READ_IN_DATA_AI():\n",
    "\n",
    "    def __init__(self, COINS, TIMEFRAME, PIPE, PATH):\n",
    "        self.PATH                               = PATH\n",
    "        self.PIPE                               = PIPE\n",
    "        self.COINS                              = COINS\n",
    "        self.TIMEFRAME                          = TIMEFRAME\n",
    "        self.LIST                               = self.READ_IN_RAW_DATA()\n",
    "\n",
    "\n",
    "    def READ_IN_RAW_DATA(self):\n",
    "        COIN_HIERARCHY = []\n",
    "\n",
    "        for a in range(len(self.COINS)):\n",
    "            TIME_FRAME_HIERARCHY = []\n",
    "            for b in range(len(self.TIMEFRAME)):\n",
    "\n",
    "                FILENAME                    = f'{self.COINS[a]} -- {self.TIMEFRAME[b]} -- {self.PIPE} -- AI TEST.csv'\n",
    "                FILE_PATH                   = self.PATH + FILENAME\n",
    "                DF                          = (pd.read_csv(FILE_PATH)).iloc[:-1]\n",
    "                DF[\"DATE\"]                  = pd.to_datetime(DF[\"DATE\"])\n",
    "\n",
    "                TIME_FRAME_HIERARCHY.append(DF)\n",
    "\n",
    "            COIN_HIERARCHY.append(TIME_FRAME_HIERARCHY)\n",
    "\n",
    "        return COIN_HIERARCHY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TIMEFRAME_RDS                               = ['1D']\n",
    "COINS                                       = ['BTCUSDT', 'ETHUSDT', 'XRPUSDT', 'DOGEUSDT', 'ADAUSDT', 'AVAXUSDT']\n",
    "\n",
    "PATH_RDS                                    = r'/Users/westhomas/Desktop/ENKI/ENKI_AI_DEVELOPMENT/__AI__DATA__/_DATA_/'\n",
    "PATH_RDF                                    = r'/Users/westhomas/Desktop/ENKI/ENKI_AI_DEVELOPMENT/__AI__DATA__/_FINAL_VIEW_/'\n",
    "\n",
    "DATA                                        = READ_IN_DATA(COINS, TIMEFRAME_RDS, PATH_RDS)\n",
    "\n",
    "\n",
    "DF_TESTING                                  = DATA.LIST[0][0].copy()\n",
    "DF_TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708e490a746748e39c63ac5af9bf1cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c61fd31ce6413581c9a1991ceb89c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 16:11:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.957102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.957073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.957041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.957010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.377500</td>\n",
       "      <td>13.956918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Close Price: |\n",
      "Actual Close Price: 96506.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "MODEL_NAME  = \"google/t5-v1_1-small\"  \n",
    "TOKENIZER   = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DATA PREPARATION FUNCTION\n",
    "# ------------------------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    \"\"\"Creates dataset for Chronos-T5 using past 20 periods as input, predicting next CLOSE price.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Define input features\n",
    "    feature_columns = [\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"RSI_14\", \"MACD\", \"BB_UPPER\", \"BB_LOWER\"]\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "    # Function to create structured string representation of past data\n",
    "    def create_feature_string(window):\n",
    "        \"\"\"Creates a structured string for model input.\"\"\"\n",
    "        return \" | \".join([f\"{col}:{round(window[col].iloc[i], 4)}\" for i in range(len(window)) for col in feature_columns])\n",
    "\n",
    "    # Create rolling window input\n",
    "    rolling_windows = df[feature_columns].rolling(window=20)\n",
    "    df[\"PAST_VALUES\"] = [create_feature_string(window) if len(window) == 20 else None for window in rolling_windows]\n",
    "\n",
    "    # Drop NaN values from rolling window operations\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Define the target as the next closing price\n",
    "    df[\"TARGET\"] = df[\"CLOSE\"].shift(-1)\n",
    "\n",
    "    # Drop last row since it has no target\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return Dataset.from_pandas(df[['PAST_VALUES', 'TARGET']])\n",
    "\n",
    "\n",
    "DATASET = prepare_dataset(DF_TESTING)\n",
    "DATASET = DATASET.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = TOKENIZER(examples[\"PAST_VALUES\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Ensure TARGET is a string before tokenization\n",
    "    examples[\"TARGET\"] = [str(x) for x in examples[\"TARGET\"]]\n",
    "    labels = TOKENIZER(examples[\"TARGET\"], padding=\"max_length\", truncation=True, max_length=10)\n",
    "    \n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "TOKENIZED_TRAIN_DATASET = DATASET[\"train\"].map(preprocess_function, batched=True)\n",
    "TOKENIZED_EVAL_DATASET = DATASET[\"test\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# Load Model\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# Training Parameters\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=\"./chronos_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainer Setup\n",
    "TRAINER = Trainer(\n",
    "    model=MODEL,\n",
    "    args=TRAINING_ARGS,\n",
    "    train_dataset=TOKENIZED_TRAIN_DATASET,\n",
    "    eval_dataset=TOKENIZED_EVAL_DATASET,\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "TRAINER.train()\n",
    "\n",
    "# Save Model\n",
    "MODEL.save_pretrained(\"./fine_tuned_chronos\")\n",
    "TOKENIZER.save_pretrained(\"./fine_tuned_chronos\")\n",
    "\n",
    "# ------------------------------\n",
    "# PREDICTION FUNCTION & COMPARISON\n",
    "# ------------------------------\n",
    "def predict_future(df, model, tokenizer):\n",
    "    \"\"\"Uses the fine-tuned model to predict the next closing price.\"\"\"\n",
    "    df = df.copy()\n",
    "    last_values = \" | \".join([\n",
    "        f\"{col}:{round(value, 4)}\" for col in df.columns if col in [\"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"RSI_14\", \"MACD\", \"BB_UPPER\", \"BB_LOWER\"]\n",
    "        for value in df[col].iloc[-20:]\n",
    "    ])\n",
    "    \n",
    "    inputs = tokenizer(last_values, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=10)\n",
    "    \n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "# Load the fine-tuned model\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(\"./fine_tuned_chronos\").to(DEVICE)\n",
    "TOKENIZER = T5Tokenizer.from_pretrained(\"./fine_tuned_chronos\")\n",
    "\n",
    "# Make Prediction & Compare with Last Known Values\n",
    "PREDICTED_VALUE = predict_future(DF_TESTING, MODEL, TOKENIZER)\n",
    "ACTUAL_VALUE = DF_TESTING.iloc[-1]['CLOSE']\n",
    "\n",
    "print(f\"Predicted Close Price: {PREDICTED_VALUE}\")\n",
    "print(f\"Actual Close Price: {ACTUAL_VALUE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "###############################################    RAW EXAMPLE    ###############################################\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "TRAIN_RATIO                     = 0.97\n",
    "COIN                            = 'BTCUSDT'\n",
    "PATH_USE                        = r'/Users/westhomas/Desktop/ENKI/ENKI_AI_DEVELOPMENT/_DATA_/'\n",
    "FILE                            = f\"{COIN} -- 1D.csv\"\n",
    "\n",
    "\n",
    "TRAINING_DATA                   = pd.read_csv(PATH_USE + FILE) \n",
    "TRAINING_DATA[\"DATE\"]           = pd.to_datetime(TRAINING_DATA[\"DATE\"], dayfirst=True)\n",
    "\n",
    "\n",
    "TRAIN_SIZE                      = int(len(TRAINING_DATA) * TRAIN_RATIO)\n",
    "TRAIN_DF                        = TRAINING_DATA.iloc[:TRAIN_SIZE].reset_index(drop=True) \n",
    "VALID_DF                        = TRAINING_DATA.iloc[TRAIN_SIZE:].reset_index(drop=True) \n",
    "\n",
    "DATA_LABELS                     = [f'{COIN}_TRAINING_DATA.json', f'{COIN}_VALIDATIO_DATA.json']\n",
    "DATA_LIST                       = [TRAIN_DF, VALID_DF]\n",
    "\n",
    "\n",
    "def EXPORT_MODEL_TRAIN_DATA(INPUT_DF, FILENAME):\n",
    "    #FEATURES_LIST = ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME', 'EMA_4', 'EMA_8', 'EMA_12', 'EMA_18', 'EMA_24', 'EMA_36', 'EMA_50', 'EMA_100', 'EMA_200']\n",
    "    \n",
    "    #INPUT_DF[\"PAST_VALUE_TEXT\"]     = INPUT_DF[FEATURES_LIST].apply(lambda row: \" | \".join([f\"{feature}: {value:.3f}\" for feature, value in zip(FEATURES_LIST, row)]), axis=1)\n",
    "    #INPUT_DF[\"TARGET_VALUE_TEXT\"]   = INPUT_DF[['TARGET']].apply(lambda row: \" | \".join([f\"TARGET : {value:.3f}\" for feature, value in zip(FEATURES_LIST, row)]), axis=1)\n",
    "\n",
    "    #CHRONOS_DATA                    = { \"START\"         : str(INPUT_DF[\"DATE\"].iloc[0]),            # First timestamp\n",
    "    #                                    \"PAST_VALUES\"   : INPUT_DF[\"PAST_VALUE_TEXT\"].tolist(),     # Text-based features\n",
    "    #                                    \"TARGET\"        : INPUT_DF[\"TARGET_VALUE_TEXT\"].tolist()}   # Convert targets to strings\n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "    EXCLUSION                       = ['DATE', 'TARGET']\n",
    "    FEATURES                        = INPUT_DF.columns.tolist()\n",
    "    FEATURES_LIST                   = [ITEM for ITEM in FEATURES if ITEM not in EXCLUSION]\n",
    "\n",
    "    CHRONOS_DATA                    = { \"START\"         : str(INPUT_DF[\"DATE\"].iloc[0]),            # First timestamp\n",
    "                                        \"PAST_VALUES\"   : INPUT_DF[FEATURES_LIST].values.tolist(),  # Input features\n",
    "                                        \"TARGET\"        : INPUT_DF[\"TARGET\"].tolist()}              # Convert targets to strings\n",
    "\n",
    "\n",
    "    PATH_USE                        = r'/Users/westhomas/Desktop/ENKI/ENKI_AI_DEVELOPMENT/_DATA_/'\n",
    "    FULL_JSON_PATH                  = os.path.join(PATH_USE, FILENAME)\n",
    "    with open(FULL_JSON_PATH, \"w\") as f: json.dump([CHRONOS_DATA], f)\n",
    "\n",
    "    return CHRONOS_DATA\n",
    "\n",
    "\n",
    "\n",
    "for A in range(len(DATA_LABELS)): OUTPUT_DATA = EXPORT_MODEL_TRAIN_DATA(DATA_LIST[A], DATA_LABELS[A])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "MODEL_NAME                          = \"google/t5-v1_1-small\"\n",
    "TOKENIZER                           = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "\n",
    "def PREPROCESS_FUNCTION(examples):\n",
    "    inputs = TOKENIZER([str(x) for x in examples[\"PAST_VALUES\"]], padding=\"max_length\", truncation=True, max_length=1280)\n",
    "    labels = TOKENIZER([str(x) for x in examples[\"TARGET\"]], padding=\"max_length\", truncation=True, max_length=1280)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATASET                 = load_dataset(\"json\", data_files={\"train\": os.path.join(PATH_USE, DATA_LABELS[0]), \"validation\": os.path.join(PATH_USE, DATA_LABELS[1])})\n",
    "TOKENIZED_TRAIN_DATASET = DATASET[\"train\"].map(PREPROCESS_FUNCTION, batched=True)\n",
    "TOKENIZED_EVAL_DATASET  = DATASET[\"validation\"].map(PREPROCESS_FUNCTION, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL.to(DEVICE)\n",
    "\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir                      = \"./chronos_model\",\n",
    "    per_device_train_batch_size     = 16,\n",
    "    num_train_epochs                = 10,\n",
    "    learning_rate                   = 1e-5,\n",
    "    weight_decay                    = 0.01,\n",
    "    save_strategy                   = \"epoch\",\n",
    "    logging_dir                     = \"./logs\",\n",
    "    logging_steps                   = 10,\n",
    "    evaluation_strategy             = \"epoch\", \n",
    "    save_total_limit                = 3,\n",
    "    load_best_model_at_end          = True,  \n",
    "    report_to                       = \"none\",\n",
    ")\n",
    "\n",
    "\n",
    "TRAINER = Trainer(\n",
    "    model                           = MODEL,\n",
    "    args                            = TRAINING_ARGS,\n",
    "    train_dataset                   = TOKENIZED_TRAIN_DATASET,\n",
    "    eval_dataset                    = TOKENIZED_EVAL_DATASET)\n",
    "\n",
    "\n",
    "TRAINER.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
